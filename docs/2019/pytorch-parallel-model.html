<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.19.1 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>How PyTorch implements DataParallel? - Blog</title>
<meta name="description" content="PyTorch can send batches and models to different GPUs automatically with DataParallel(model). How is it possible? I assume you know PyTorch uses dynamic computational graph as well as Python GIL. And PyTorch version is v1.0.1.  ">


  <meta name="author" content="Erick Guan">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Blog">
<meta property="og:title" content="How PyTorch implements DataParallel?">
<meta property="og:url" content="https://erickguan.me/2019/pytorch-parallel-model">


  <meta property="og:description" content="PyTorch can send batches and models to different GPUs automatically with DataParallel(model). How is it possible? I assume you know PyTorch uses dynamic computational graph as well as Python GIL. And PyTorch version is v1.0.1.  ">



  <meta property="og:image" content="https://erickguan.me/assets/images/2019/Pytorch_logo-200.png">



  <meta name="twitter:site" content="@erickguan">
  <meta name="twitter:title" content="How PyTorch implements DataParallel?">
  <meta name="twitter:description" content="PyTorch can send batches and models to different GPUs automatically with DataParallel(model). How is it possible? I assume you know PyTorch uses dynamic computational graph as well as Python GIL. And PyTorch version is v1.0.1.  ">
  <meta name="twitter:url" content="https://erickguan.me/2019/pytorch-parallel-model">

  
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="https://erickguan.me/assets/images/2019/Pytorch_logo-200.png">
  

  
    <meta name="twitter:creator" content="@erickguan">
  



  <meta property="article:published_time" content="2019-03-05T00:00:00+01:00">





  

  


<link rel="canonical" href="https://erickguan.me/2019/pytorch-parallel-model">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Erick Guan",
      "url": "https://erickguan.me/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Blog Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#da532c">
<meta name="apple-mobile-web-app-title" content="Erick Guan">
<meta name="application-name" content="Erick Guan">
<meta name="theme-color" content="#ffffff">

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Blog
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about">About</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero"
  style=" background-image: url('');"
>
  
    <img src="/assets/images/2019/Pytorch_logo-1400.png" alt="How PyTorch implements DataParallel?" class="page__hero-image">
  
  
    <span class="page__hero-caption">Photo from <a href="https://commons.wikimedia.org/wiki/File:Pytorch_logo.png">WikiCommons</a>
</span>
  
</div>





<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="How PyTorch implements DataParallel?">
    <meta itemprop="description" content="PyTorch can send batches and models to different GPUs automatically with DataParallel(model). How is it possible? I assume you know PyTorch uses dynamic computational graph as well as Python GIL. And PyTorch version is v1.0.1.">
    <meta itemprop="datePublished" content="2019-03-05T00:00:00+01:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">How PyTorch implements DataParallel?
</h1>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On This Page</h4></header>
              <ul class="toc__menu">
  <li><a href="#dataparallel-interface">DataParallel interface</a>
    <ul>
      <li><a href="#and-how-function-works">And how Function works</a></li>
      <li><a href="#step-1-and-step-2-split-minibatch-on-gpu0-and-move-to-gpu">Step 1 and Step 2: split minibatch on GPU:0 and move to GPU</a></li>
      <li><a href="#step-3-copy-models-to-gpu">Step 3: copy models to GPU</a></li>
      <li><a href="#step-4-forward-pass">Step 4: Forward pass</a></li>
      <li><a href="#step-5-compute">Step 5: Compute</a></li>
      <li><a href="#step-6-loss-value">Step 6: Loss value</a></li>
    </ul>
  </li>
  <li><a href="#discussion">Discussion</a></li>
</ul>

            </nav>
          </aside>
        
        <p>PyTorch can send batches and models to different GPUs automatically with <code class="language-plaintext highlighter-rouge">DataParallel(model)</code>. How is it possible? I assume you know PyTorch uses dynamic computational graph as well as Python GIL. And PyTorch version is v1.0.1.</p>

<p>This is a complicated question and I asked on the PyTorch forum. I got <a href="https://discuss.pytorch.org/t/how-pytorchs-parallel-method-and-distributed-method-works/30349/2?u=fantasticfears">a reply</a> from <a href="https://discuss.pytorch.org/u/rasbt">Sebastian Raschka</a>.</p>

<p><img src="/assets/images/2018/dataparallel.png" alt="Data parallel’s process in a high level" /></p>

<p>TL;DR:</p>

<p>PyTorch trys hard in zero-copying. <code class="language-plaintext highlighter-rouge">DataParallel</code> splits tensor by its total size instead of along any axis.</p>

<h1 id="dataparallel-interface"><code class="language-plaintext highlighter-rouge">DataParallel</code> interface</h1>

<p><code class="language-plaintext highlighter-rouge">Module</code> defines its constructor and <code class="language-plaintext highlighter-rouge">forward</code> function. And <code class="language-plaintext highlighter-rouge">DataParallel</code> does the same. Let’s focus on <code class="language-plaintext highlighter-rouge">forward</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># ...
</span>    <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">)</span> <span class="c1"># Step 1, 2
</span>    <span class="c1"># ...
</span>    <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># Build graph
</span>    <span class="n">replicas</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replicate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)])</span> <span class="c1"># Step 3
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parallel_apply</span><span class="p">(</span><span class="n">replicas</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span> <span class="c1"># Step 4
</span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_device</span><span class="p">)</span> <span class="c1"># Step 5
</span></code></pre></div></div>

<p>There are 4 methods extracted and defined as instance methods which are <code class="language-plaintext highlighter-rouge">scatter</code>, <code class="language-plaintext highlighter-rouge">replicate</code>, <code class="language-plaintext highlighter-rouge">gather</code> and <code class="language-plaintext highlighter-rouge">parallel_apply</code>. They executes the step 1, 2; step 3; step 4; step 5 respectively.</p>

<p>Firstly, we have to recognize PyTorch utilize dynamic computational graph. That’s why module is invoked to build nodes. It builds the graph, parameters, grads and buffers.</p>

<h2 id="and-how-function-works">And how <code class="language-plaintext highlighter-rouge">Function</code> works</h2>

<p>Before we start to look into diffierent steps, let’s look at <code class="language-plaintext highlighter-rouge">Function</code>. We are going to see it many times.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Function</span><span class="p">(</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">FunctionMeta</span><span class="p">,</span> <span class="n">_C</span><span class="o">.</span><span class="n">_FunctionBase</span><span class="p">,</span> <span class="n">_ContextMethodMixin</span><span class="p">,</span> <span class="n">_HookMixin</span><span class="p">)):</span>
    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="s">r"""...
        It must accept a context ctx as the first argument, followed by any
        number of arguments (tensors or other types).

        The context can be used to store tensors that can be then retrieved
        during the backward pass.
        """</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">grad_outputs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span>
</code></pre></div></div>

<p>Some helper functions used <code class="language-plaintext highlighter-rouge">Function</code> as the base class. Moreover, since you will see <code class="language-plaintext highlighter-rouge">Function.apply</code> a lot in the following, we should see how it works. (<code class="language-plaintext highlighter-rouge">metaclass</code> is tricky.)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">with_metaclass</span><span class="p">(</span><span class="n">meta</span><span class="p">,</span> <span class="o">*</span><span class="n">bases</span><span class="p">):</span>
    <span class="s">"""Create a base class with a metaclass."""</span>
    <span class="c1"># This requires a bit of explanation: the basic idea is to make a dummy
</span>    <span class="c1"># metaclass for one level of class instantiation that replaces itself with
</span>    <span class="c1"># the actual metaclass.
</span>    <span class="k">class</span> <span class="nc">metaclass</span><span class="p">(</span><span class="n">meta</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">__new__</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">this_bases</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">meta</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">bases</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">type</span><span class="o">.</span><span class="n">__new__</span><span class="p">(</span><span class="n">metaclass</span><span class="p">,</span> <span class="s">'temporary_class'</span><span class="p">,</span> <span class="p">(),</span> <span class="p">{})</span>
</code></pre></div></div>

<p>This is tricky. We have to start from Python’s object design. Starting from Python 3, everything is an object. And all objects are constructed by <code class="language-plaintext highlighter-rouge">class type(name, bases, dict)</code>. Each parameter would become class object’s <code class="language-plaintext highlighter-rouge">__name__</code>, <code class="language-plaintext highlighter-rouge">__bases__</code>, and <code class="language-plaintext highlighter-rouge">__dict__</code>. So <code class="language-plaintext highlighter-rouge">type</code> is the dynamic way to build a class. <code class="language-plaintext highlighter-rouge">with_metaclass</code> is a helper to build a metaclass without defining <code class="language-plaintext highlighter-rouge">__dict__</code> yet. Namely, <code class="language-plaintext highlighter-rouge">with_metaclass(FunctionMeta, _C._FunctionBase, _ContextMethodMixin, _HookMixin))</code> reads <code class="language-plaintext highlighter-rouge">FunctionMeta</code> is the metaclass but the bases are in the tuple.</p>

<p>Then we are not far away from how it works.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FunctionMeta</span><span class="p">(</span><span class="nb">type</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">bases</span><span class="p">,</span> <span class="n">attrs</span><span class="p">):</span>
        <span class="o">//</span> <span class="o">...</span>
        <span class="n">backward_fn</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">name</span> <span class="o">+</span> <span class="s">'Backward'</span><span class="p">,</span> <span class="p">(</span><span class="n">BackwardCFunction</span><span class="p">,),</span> <span class="p">{</span><span class="s">'_forward_cls'</span><span class="p">:</span> <span class="n">cls</span><span class="p">})</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="s">'_backward_cls'</span><span class="p">,</span> <span class="n">backward_fn</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">FunctionMeta</span><span class="p">,</span> <span class="n">cls</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">bases</span><span class="p">,</span> <span class="n">attrs</span><span class="p">)</span>

</code></pre></div></div>

<p>Basically a <code class="language-plaintext highlighter-rouge">backward_fn</code> is defined and stored in <code class="language-plaintext highlighter-rouge">_backward_cls</code>. Its base class is simple.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BackwardCFunction</span><span class="p">(</span><span class="n">_C</span><span class="o">.</span><span class="n">_FunctionBase</span><span class="p">,</span> <span class="n">_ContextMethodMixin</span><span class="p">,</span> <span class="n">_HookMixin</span><span class="p">):</span>
    <span class="c1"># ...
</span>    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_cls</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
</code></pre></div></div>

<p>It will invoke the <code class="language-plaintext highlighter-rouge">Function</code> class’s <code class="language-plaintext highlighter-rouge">backward</code> method. Therefore <code class="language-plaintext highlighter-rouge">_C._FunctionBase</code> is more interesting.  <code class="language-plaintext highlighter-rouge">C._FunctionBase</code> is defined by <code class="language-plaintext highlighter-rouge">THPFunctionType</code>. And the <code class="language-plaintext highlighter-rouge">THPFunction</code> defines its data structure.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="k">struct</span> <span class="n">PyMethodDef</span> <span class="n">THPFunction_methods</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span>
  <span class="p">{(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="s">"apply"</span><span class="p">,</span> <span class="p">(</span><span class="n">PyCFunction</span><span class="p">)</span><span class="n">THPFunction_apply</span><span class="p">,</span> <span class="n">METH_CLASS</span> <span class="o">|</span> <span class="n">METH_VARARGS</span><span class="p">,</span> <span class="nb">nullptr</span><span class="p">},</span>
  <span class="p">{(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="s">"_do_forward"</span><span class="p">,</span> <span class="p">(</span><span class="n">PyCFunction</span><span class="p">)</span><span class="n">THPFunction_do_forward</span><span class="p">,</span> <span class="n">METH_VARARGS</span><span class="p">,</span> <span class="nb">nullptr</span><span class="p">},</span>
  <span class="p">{(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="s">"_do_backward"</span><span class="p">,</span> <span class="p">(</span><span class="n">PyCFunction</span><span class="p">)</span><span class="n">THPFunction_do_backward</span><span class="p">,</span> <span class="n">METH_VARARGS</span><span class="p">,</span> <span class="nb">nullptr</span><span class="p">},</span>
  <span class="p">{(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="s">"_register_hook_dict"</span><span class="p">,</span> <span class="p">(</span><span class="n">PyCFunction</span><span class="p">)</span><span class="n">THPFunction__register_hook_dict</span><span class="p">,</span> <span class="n">METH_O</span><span class="p">,</span> <span class="nb">nullptr</span><span class="p">},</span>
  <span class="p">{(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="s">"register_hook"</span><span class="p">,</span> <span class="p">(</span><span class="n">PyCFunction</span><span class="p">)</span><span class="n">THPFunction_register_hook</span><span class="p">,</span> <span class="n">METH_O</span><span class="p">,</span> <span class="nb">nullptr</span><span class="p">},</span>
  <span class="p">{</span><span class="nb">nullptr</span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div></div>

<p>It defines the <code class="language-plaintext highlighter-rouge">apply</code> method. That’s something we are interested.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">PyObject</span> <span class="o">*</span><span class="nf">THPFunction_apply</span><span class="p">(</span><span class="n">PyObject</span> <span class="o">*</span><span class="n">cls</span><span class="p">,</span> <span class="n">PyObject</span> <span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
<span class="p">{</span>
  <span class="c1">// ...</span>
  <span class="n">THPObjectPtr</span> <span class="n">backward_cls</span><span class="p">(</span><span class="n">PyObject_GetAttrString</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="s">"_backward_cls"</span><span class="p">));</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">backward_cls</span><span class="p">)</span> <span class="k">return</span> <span class="nb">nullptr</span><span class="p">;</span>
  <span class="n">THPObjectPtr</span> <span class="n">ctx_obj</span><span class="p">(</span><span class="n">PyObject_CallFunctionObjArgs</span><span class="p">(</span><span class="n">backward_cls</span><span class="p">,</span> <span class="nb">nullptr</span><span class="p">));</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">ctx_obj</span><span class="p">)</span> <span class="k">return</span> <span class="nb">nullptr</span><span class="p">;</span>
  <span class="n">THPFunction</span><span class="o">*</span> <span class="n">ctx</span> <span class="o">=</span> <span class="p">(</span><span class="n">THPFunction</span><span class="o">*</span><span class="p">)</span><span class="n">ctx_obj</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>

  <span class="c1">// Prepare inputs and allocate context (grad fn)</span>
  <span class="k">auto</span> <span class="n">info_pair</span> <span class="o">=</span> <span class="n">unpack_input</span><span class="o">&lt;</span><span class="nb">false</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
  <span class="n">UnpackedInput</span><span class="o">&amp;</span> <span class="n">unpacked_input</span> <span class="o">=</span> <span class="n">info_pair</span><span class="p">.</span><span class="n">first</span><span class="p">;</span>
  <span class="n">InputFlags</span><span class="o">&amp;</span> <span class="n">input_info</span> <span class="o">=</span> <span class="n">info_pair</span><span class="p">.</span><span class="n">second</span><span class="p">;</span>

  <span class="c1">// Record input nodes if tracing</span>
  <span class="c1">// ...</span>

  <span class="c1">// Initialize backward function (and ctx)</span>
  <span class="kt">bool</span> <span class="n">is_executable</span> <span class="o">=</span> <span class="n">input_info</span><span class="p">.</span><span class="n">is_executable</span><span class="p">;</span>
  <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">cdata</span><span class="p">.</span><span class="n">set_next_edges</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">input_info</span><span class="p">.</span><span class="n">next_edges</span><span class="p">));</span>
  <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">needs_input_grad</span> <span class="o">=</span> <span class="n">input_info</span><span class="p">.</span><span class="n">needs_input_grad</span><span class="p">.</span><span class="n">release</span><span class="p">();</span>
  <span class="n">ctx</span><span class="o">-&gt;</span><span class="n">is_variable_input</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">input_info</span><span class="p">.</span><span class="n">is_variable_input</span><span class="p">);</span>

  <span class="c1">// Prepend ctx to input_tuple, in preparation for static method call</span>
  <span class="k">auto</span> <span class="n">num_args</span> <span class="o">=</span> <span class="n">PyTuple_GET_SIZE</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
  <span class="n">THPObjectPtr</span> <span class="n">ctx_input_tuple</span><span class="p">(</span><span class="n">PyTuple_New</span><span class="p">(</span><span class="n">num_args</span> <span class="o">+</span> <span class="mi">1</span><span class="p">));</span>
  <span class="n">PyTuple_SET_ITEM</span><span class="p">(</span><span class="n">ctx_input_tuple</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ctx_obj</span><span class="p">.</span><span class="n">release</span><span class="p">());</span>
  <span class="c1">// ...</span>

  <span class="c1">// Call forward</span>
  <span class="n">THPObjectPtr</span> <span class="n">tensor_outputs</span><span class="p">;</span>
  <span class="p">{</span>
    <span class="n">AutoGradMode</span> <span class="n">grad_mode</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span>
    <span class="n">THPObjectPtr</span> <span class="n">forward_fn</span><span class="p">(</span><span class="n">PyObject_GetAttrString</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="s">"forward"</span><span class="p">));</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">forward_fn</span><span class="p">)</span> <span class="k">return</span> <span class="nb">nullptr</span><span class="p">;</span>
    <span class="n">tensor_outputs</span> <span class="o">=</span> <span class="n">PyObject_CallObject</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="n">ctx_input_tuple</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">tensor_outputs</span><span class="p">)</span> <span class="k">return</span> <span class="nb">nullptr</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">return</span> <span class="n">process_outputs</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">ctx</span><span class="p">,</span> <span class="n">unpacked_input</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">tensor_outputs</span><span class="p">),</span>
                         <span class="n">is_executable</span><span class="p">,</span> <span class="n">node</span><span class="p">);</span>
  <span class="n">END_HANDLE_TH_ERRORS</span>
<span class="p">}</span>
</code></pre></div></div>

<p>From here, we know that the <code class="language-plaintext highlighter-rouge">cls.apply</code> invokes <code class="language-plaintext highlighter-rouge">cls.forward</code> and prepares information for <code class="language-plaintext highlighter-rouge">cls.backward</code>. <code class="language-plaintext highlighter-rouge">cls.apply</code> takes its own class information and all parameters from Python. And those parameters will be applied on <code class="language-plaintext highlighter-rouge">cls.forward</code>. Noticabily, <code class="language-plaintext highlighter-rouge">ctx</code> object can carry the information to backward pass.</p>

<p>Now we can explore how  <code class="language-plaintext highlighter-rouge">DataParallel</code> works.</p>

<h2 id="step-1-and-step-2-split-minibatch-on-gpu0-and-move-to-gpu">Step 1 and Step 2: split minibatch on GPU:0 and move to GPU</h2>

<p><code class="language-plaintext highlighter-rouge">scatter</code> scatters the input argugments and returns a tuple of <code class="language-plaintext highlighter-rouge">inputs</code> and <code class="language-plaintext highlighter-rouge">kwargs</code>. The actual definiton is here:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">scatter_kwargs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">target_gpus</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="s">r"""Scatter with support for kwargs dictionary"""</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_gpus</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">inputs</span> <span class="k">else</span> <span class="p">[]</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">target_gpus</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="p">[]</span>
    <span class="c1"># ...
</span>    <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">scatter</code> function is a recursive closure unwrap its input tensor(s). But its core is the <code class="language-plaintext highlighter-rouge">Scatter</code> class. This is a <a href="https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html"><code class="language-plaintext highlighter-rouge">Function</code> class</a> which will operate differently in <code class="language-plaintext highlighter-rouge">forward</code> and <code class="language-plaintext highlighter-rouge">backward</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Scatter</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">target_gpus</span><span class="p">,</span> <span class="n">chunk_sizes</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="c1"># ...
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target_gpus</span><span class="p">,</span> <span class="n">chunk_sizes</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">streams</span><span class="p">)</span>
        <span class="c1"># Synchronize with the copy stream
</span>        <span class="k">if</span> <span class="n">streams</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">output</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">target_gpus</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
                    <span class="n">main_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>
                    <span class="n">main_stream</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="n">streams</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="n">output</span><span class="o">.</span><span class="n">record_stream</span><span class="p">(</span><span class="n">main_stream</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">grad_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">Gather</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">input_device</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="o">*</span><span class="n">grad_output</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s focus on forward path. The backward path should be easier to understand. Thus on a multi-gpu environment, the input tensor will be sent to a PyTorch module for scattering and copying. This process needs synchronization.</p>

<p>The actual works in done by C++.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">scatter</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">tensor</span><span class="p">,</span>
    <span class="n">at</span><span class="o">::</span><span class="n">IntList</span> <span class="n">devices</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;&gt;&amp;</span> <span class="n">chunk_sizes</span><span class="p">,</span> <span class="c1">// optional parameter is bad</span>
    <span class="kt">int64_t</span> <span class="n">dim</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="o">&gt;&gt;&gt;&amp;</span> <span class="n">streams</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">chunks</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">chunk_sizes</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// ...</span>
    <span class="c1">// flatten tensor by size. The default uses the other path</span>
    <span class="kt">int64_t</span> <span class="n">chunk_start</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">chunk</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">chunk</span> <span class="o">&lt;</span> <span class="n">chunk_sizes</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">chunk</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">chunk_size</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">chunk_sizes</span><span class="p">)[</span><span class="n">chunk</span><span class="p">];</span>
      <span class="n">AT_CHECK</span><span class="p">(</span><span class="n">chunk_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s">"Chunk size must be positive"</span><span class="p">);</span>
      <span class="n">chunks</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">narrow</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">chunk_start</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">));</span>
      <span class="n">chunk_start</span> <span class="o">+=</span> <span class="n">chunk_size</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="c1">// usually a tensor is seperated into chunks among devices</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="n">chunk</span><span class="p">(</span><span class="cm">/*chunks=*/</span><span class="n">devices</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="cm">/*dim=*/</span><span class="n">dim</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">OptionalCUDAStreamGuard</span> <span class="n">cuda_guard</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">chunk</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">chunk</span> <span class="o">&lt;</span> <span class="n">chunks</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">chunk</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// ...</span>
    <span class="c1">// dispatch to different devices</span>
    <span class="n">chunks</span><span class="p">[</span><span class="n">chunk</span><span class="p">]</span> <span class="o">=</span> <span class="n">chunks</span><span class="p">[</span><span class="n">chunk</span><span class="p">].</span><span class="n">contiguous</span><span class="p">().</span><span class="n">to</span><span class="p">(</span>
        <span class="p">{</span><span class="n">at</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CUDA</span><span class="p">,</span> <span class="n">device_index</span><span class="p">},</span> <span class="cm">/*non_blocking=*/</span><span class="nb">true</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">chunks</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>As a matter of fact, tensor would be split which is done by <code class="language-plaintext highlighter-rouge">at::narrow</code> in the end. Since the operation only happens to strides and sizes, the memory is reused! PyTorch takes zero copy seriously at every level. But an important insight is that tensor is splitted regardless of its shape. You need to align different input tensors by its total size instead of a particular dimension.</p>

<h2 id="step-3-copy-models-to-gpu">Step 3: copy models to GPU</h2>

<p><code class="language-plaintext highlighter-rouge">replicate</code> sounds easy! But no. So the module is passed in as <code class="language-plaintext highlighter-rouge">network</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">replicate</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">devices</span><span class="p">,</span> <span class="n">detach</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">._functions</span> <span class="kn">import</span> <span class="n">Broadcast</span>

    <span class="o">//</span> <span class="o">...</span>
    <span class="n">num_replicas</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span>

    <span class="o">//</span> <span class="n">copy</span> <span class="n">model</span> <span class="n">parameters</span>
    <span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="n">param_indices</span> <span class="o">=</span> <span class="p">{</span><span class="n">param</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">params</span><span class="p">)}</span>
    <span class="n">param_copies</span> <span class="o">=</span> <span class="n">Broadcast</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">devices</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">param_copies</span> <span class="o">=</span> <span class="p">[</span><span class="n">param_copies</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)]</span>
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_copies</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">))]</span>

    <span class="o">//</span> <span class="n">copy</span> <span class="n">model</span> <span class="n">buffers</span>
    <span class="n">buffers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">buffers</span><span class="p">())</span>
    <span class="n">buffer_indices</span> <span class="o">=</span> <span class="p">{</span><span class="n">buf</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">buffers</span><span class="p">)}</span>
    <span class="n">buffer_copies</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">broadcast_coalesced</span><span class="p">(</span><span class="n">buffers</span><span class="p">,</span> <span class="n">devices</span><span class="p">)</span>

    <span class="o">//</span> <span class="n">copy</span> <span class="n">model</span> <span class="n">modules</span>
    <span class="n">modules</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">modules</span><span class="p">())</span>
    <span class="n">module_copies</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">devices</span><span class="p">]</span>
    <span class="n">module_indices</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="o">//</span> <span class="n">construrct</span> <span class="n">modules</span>
    <span class="o">//</span> <span class="o">...</span>

    <span class="o">//</span> <span class="n">copy</span> <span class="n">every</span> <span class="n">submodules</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">modules</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="o">//</span> <span class="o">...</span>
                    <span class="n">replica</span> <span class="o">=</span> <span class="n">module_copies</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
                    <span class="n">replica</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">module_copies</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">module_idx</span><span class="p">]</span>
        <span class="o">//</span> <span class="ow">and</span> <span class="n">their</span> <span class="n">parameters</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="o">//</span> <span class="o">...</span>
                    <span class="n">replica</span> <span class="o">=</span> <span class="n">module_copies</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
                    <span class="n">replica</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_copies</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">param_idx</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> \
                        <span class="k">if</span> <span class="n">detach</span> <span class="k">else</span> <span class="n">param_copies</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">param_idx</span><span class="p">]</span>
        <span class="o">//</span> <span class="n">also</span> <span class="n">buffers</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_buffers</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="o">//</span> <span class="o">...</span>
                    <span class="n">replica</span> <span class="o">=</span> <span class="n">module_copies</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
                    <span class="n">replica</span><span class="o">.</span><span class="n">_buffers</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">buffer_copies</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">buffer_idx</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">module_copies</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_replicas</span><span class="p">)]</span>
</code></pre></div></div>

<p>The copying part to GPU is hard to be found. But we know they are <code class="language-plaintext highlighter-rouge">Broadcast</code> and <code class="language-plaintext highlighter-rouge">comm.broadcast_coalesced(buffers, devices)</code>.
Until this point, we have the model living in different GPUs.</p>

<p>Here is the <code class="language-plaintext highlighter-rouge">Broadcast</code>. The invocation starts with <code class="language-plaintext highlighter-rouge">Broadcast.apply(devices, *params)</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Broadcast</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">target_gpus</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="o">//</span> <span class="o">...</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">target_gpus</span> <span class="o">=</span> <span class="n">target_gpus</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">()</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">num_inputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">input_device</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">broadcast_coalesced</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">target_gpus</span><span class="p">)</span>
        <span class="o">//</span> <span class="o">...</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">t</span> <span class="k">for</span> <span class="n">tensors</span> <span class="ow">in</span> <span class="n">outputs</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">])</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">grad_outputs</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">None</span><span class="p">,)</span> <span class="o">+</span> <span class="n">ReduceAddCoalesced</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">input_device</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">num_inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">grad_outputs</span><span class="p">)</span>
</code></pre></div></div>

<p>And its backward direction <code class="language-plaintext highlighter-rouge">ReduceAddCoalesced</code> looks similar.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ReduceAddCoalesced</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">destination</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="o">*</span><span class="n">grads</span><span class="p">):</span>
        <span class="o">//</span> <span class="o">...</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">grads</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">num_inputs</span><span class="p">]</span>
                 <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">grads</span><span class="p">),</span> <span class="n">num_inputs</span><span class="p">)]</span>
        <span class="k">return</span> <span class="n">comm</span><span class="o">.</span><span class="n">reduce_add_coalesced</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">destination</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">grad_outputs</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,)</span> <span class="o">+</span> <span class="n">Broadcast</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">target_gpus</span><span class="p">,</span> <span class="o">*</span><span class="n">grad_outputs</span><span class="p">)</span>
</code></pre></div></div>

<p>So the core parts are <code class="language-plaintext highlighter-rouge">comm.broadcast_coalesced</code> and <code class="language-plaintext highlighter-rouge">comm.reduce_add_coalesced</code>.</p>

<p>The <code class="language-plaintext highlighter-rouge">comm.broadcast_coalesced</code> is implemented in C++. We have model parameters as <code class="language-plaintext highlighter-rouge">tensors</code>.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor_list2d</span> <span class="nf">broadcast_coalesced</span><span class="p">(</span><span class="n">TensorList</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">IntArrayRef</span> <span class="n">devices</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">buffer_size</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">std</span><span class="o">::</span><span class="n">all_of</span><span class="p">(</span><span class="n">tensors</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">tensors</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span>
                   <span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="k">const</span> <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">t</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="n">t</span><span class="p">.</span><span class="n">get_device</span><span class="p">()</span> <span class="o">==</span> <span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span> <span class="p">}))</span> <span class="p">{</span>
    <span class="k">throw</span> <span class="n">std</span><span class="o">::</span><span class="n">runtime_error</span><span class="p">(</span><span class="s">"all tensors must be on devices[0]"</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="c1">// ...</span>

  <span class="n">tensor_list2d</span> <span class="n">outputs</span><span class="p">(</span><span class="n">devices</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
  <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">.</span><span class="n">vec</span><span class="p">();</span>
  <span class="c1">// ...</span>

  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span> <span class="n">chunk</span> <span class="o">:</span> <span class="n">utils</span><span class="o">::</span><span class="n">take_tensors</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">))</span> <span class="p">{</span>
    <span class="c1">// ...</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">results</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">chunk</span><span class="p">.</span><span class="n">type</span><span class="p">().</span><span class="n">is_sparse</span><span class="p">())</span> <span class="p">{</span>
      <span class="k">auto</span> <span class="n">flat_tuple</span> <span class="o">=</span> <span class="n">utils</span><span class="o">::</span><span class="n">flatten_sparse_tensors</span><span class="p">(</span><span class="n">chunk</span><span class="p">.</span><span class="n">tensors</span><span class="p">);</span>
      <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">broadcast_indices</span> <span class="o">=</span> <span class="n">broadcast</span><span class="p">(</span><span class="n">flat_tuple</span><span class="p">.</span><span class="n">first</span><span class="p">,</span> <span class="n">devices</span><span class="p">);</span>
      <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">broadcast_values</span> <span class="o">=</span> <span class="n">broadcast</span><span class="p">(</span><span class="n">flat_tuple</span><span class="p">.</span><span class="n">second</span><span class="p">,</span> <span class="n">devices</span><span class="p">);</span>
      <span class="c1">// ...</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">results</span> <span class="o">=</span> <span class="n">broadcast</span><span class="p">(</span><span class="n">utils</span><span class="o">::</span><span class="n">flatten_dense_tensors</span><span class="p">(</span><span class="n">chunk</span><span class="p">.</span><span class="n">tensors</span><span class="p">),</span>
                                              <span class="n">devices</span><span class="p">);</span>
      <span class="c1">// ...</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="c1">// ...</span>
  <span class="k">return</span> <span class="n">outputs</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This is too complicated and perhaps not insightful for our aim. But effecitively NCCL is used to broadcast tensor’s address to multiple GPUs. <code class="language-plaintext highlighter-rouge">reduce_add_coalesced</code> does the reverse.</p>

<h2 id="step-4-forward-pass">Step 4: Forward pass</h2>

<p>Finally, this step is obvious. But we need to pay attention to the output tuple because of the next step.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">parallel_apply</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs_tup</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="s">r"""Applies each `module` in :attr:`modules` in parallel on arguments
    contained in :attr:`inputs` (positional) and :attr:`kwargs_tup` (keyword)
    on each of :attr:`devices`.

    Args:
        modules (Module): modules to be parallelized
        inputs (tensor): inputs to the modules
        devices (list of int or torch.device): CUDA devices

    :attr:`modules`, :attr:`inputs`, :attr:`kwargs_tup` (if given), and
    :attr:`devices` (if given) should all have same length. Moreover, each
    element of :attr:`inputs` can either be a single object as the only argument
    to a module, or a collection of positional arguments.
    """</span>
    <span class="c1"># ...
</span>
    <span class="k">def</span> <span class="nf">_worker</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># ...
</span>        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
                <span class="c1"># this also avoids accidental slicing of `input` if it is a Tensor
</span>                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                    <span class="nb">input</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,)</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">lock</span><span class="p">:</span>
                <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">output</span>
        <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">lock</span><span class="p">:</span>
                <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">e</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">threads</span> <span class="o">=</span> <span class="p">[</span><span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">_worker</span><span class="p">,</span>
                                    <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device</span><span class="p">))</span>
                   <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="ow">in</span>
                   <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs_tup</span><span class="p">,</span> <span class="n">devices</span><span class="p">))]</span>

        <span class="k">for</span> <span class="n">thread</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
            <span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">thread</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
            <span class="n">thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_worker</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">modules</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">kwargs_tup</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># ...
</span>    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">results</code> keeps track of results from every devices. Most cases, it will be a scalar loss tensor.</p>

<p>Python’s threading library is used to send things to models on different GPUs. This is a little bit expensive than I thought because of the overhead from threading.</p>

<h2 id="step-5-compute">Step 5: Compute</h2>

<p>The <code class="language-plaintext highlighter-rouge">gather</code> has a similiar design as <code class="language-plaintext highlighter-rouge">scatter</code>. The core is <code class="language-plaintext highlighter-rouge">Gather.apply(target_device, dim, *outputs)</code>.  And its core is <code class="language-plaintext highlighter-rouge">comm.gather(inputs, ctx.dim, ctx.target_device)</code>.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="nf">gather</span><span class="p">(</span>
    <span class="n">at</span><span class="o">::</span><span class="n">TensorList</span> <span class="n">tensors</span><span class="p">,</span>
    <span class="kt">int64_t</span> <span class="n">dim</span><span class="p">,</span>
    <span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">destination_index</span><span class="p">)</span> <span class="p">{</span>
  <span class="cp"># ...
</span>  <span class="n">AT_CHECK</span><span class="p">(</span><span class="o">!</span><span class="n">tensors</span><span class="p">.</span><span class="n">empty</span><span class="p">(),</span> <span class="s">"Expected at least one tensor to gather from"</span><span class="p">);</span>
  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">result</span><span class="p">;</span>
  <span class="kt">int64_t</span> <span class="n">total_size</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">first</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
  <span class="k">const</span> <span class="k">auto</span> <span class="n">first_size</span> <span class="o">=</span> <span class="n">first</span><span class="p">.</span><span class="n">sizes</span><span class="p">();</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span> <span class="n">expected_size</span><span class="p">(</span><span class="n">first_size</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">first_size</span><span class="p">.</span><span class="n">end</span><span class="p">());</span>
  <span class="k">for</span> <span class="p">(</span><span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="n">tensor</span> <span class="o">:</span> <span class="n">tensors</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// ...</span>
    <span class="n">expected_size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="n">dim</span><span class="p">);</span>
    <span class="c1">// ...</span>
    <span class="n">total_size</span> <span class="o">+=</span> <span class="n">tensor</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="n">dim</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="n">expected_size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_size</span><span class="p">;</span>
  <span class="n">at</span><span class="o">::</span><span class="n">Device</span> <span class="n">device</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">destination_index</span> <span class="o">||</span> <span class="o">*</span><span class="n">destination_index</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CUDA</span><span class="p">,</span> <span class="n">destination_index</span> <span class="o">?</span> <span class="o">*</span><span class="n">destination_index</span> <span class="o">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">empty</span><span class="p">(</span><span class="n">expected_size</span><span class="p">,</span> <span class="n">first</span><span class="p">.</span><span class="n">options</span><span class="p">().</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">));</span>

  <span class="kt">int64_t</span> <span class="n">chunk_start</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="n">tensor</span> <span class="o">:</span> <span class="n">tensors</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">result</span><span class="p">.</span><span class="n">narrow</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">chunk_start</span><span class="p">,</span> <span class="n">tensor</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
        <span class="p">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="cm">/*non_blocking=*/</span><span class="nb">true</span><span class="p">);</span>
    <span class="n">chunk_start</span> <span class="o">+=</span> <span class="n">tensor</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="n">dim</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">result</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>It’s actually hard to find things. This basically compose a final tensor from tensors from devices. <code class="language-plaintext highlighter-rouge">destination_index</code> is the index of output device which is <code class="language-plaintext highlighter-rouge">GPU:0</code> by default (defined as the parameter of <code class="language-plaintext highlighter-rouge">data_parrallel</code>).</p>

<h2 id="step-6-loss-value">Step 6: Loss value</h2>

<p>This is the standard code you will write.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p>In next minibatch, same process will be sent to GPU.</p>

<h1 id="discussion">Discussion</h1>

<p>PyTorch focuses on abstraction and zero-copy. It’s more intutive to Tensorflow 1.x. It’s indeed an engineering success. Despite its simple interfaces, we need to understand its <code class="language-plaintext highlighter-rouge">Tensor</code> abstraction and those convenient helpers. Since CUDA uses different programming models, the work to hide CUDA details is enormous. PyTorch did this by mixing C++ and Python with <em>pybind11</em>. And it strives to maintain a similiar language in a large hierarchy that aligns the <code class="language-plaintext highlighter-rouge">Tensor</code>, <code class="language-plaintext highlighter-rouge">Function</code>, and <code class="language-plaintext highlighter-rouge">Model</code>. That’s remarkable.</p>

<p>Though, programming in a mixed languages are difficult. Type checking is always complicated. GIL needs to be remembered. With CUDA, synchronization, memory management and communication expands the scope of engineering work.</p>


        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#programming" class="page__taxonomy-item" rel="tag">Programming</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#python" class="page__taxonomy-item" rel="tag">Python</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#pytorch" class="page__taxonomy-item" rel="tag">PyTorch</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2019-03-05T00:00:00+01:00">March 5, 2019</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?via=erickguan&text=How+PyTorch+implements+DataParallel%3F%20https%3A%2F%2Ferickguan.me%2F2019%2Fpytorch-parallel-model" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ferickguan.me%2F2019%2Fpytorch-parallel-model" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ferickguan.me%2F2019%2Fpytorch-parallel-model" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/2018/how-to-have-a-data-science-lab" class="pagination--pager" title="How to have a data science lab in 13 steps?
">Previous</a>
    
    
      <a href="/2019/a-decade-with-wikipedia" class="pagination--pager" title="A decade with Wikipedia
">Next</a>
    
  </nav>

    </div>

    
      <div class="page__comments">
  
  
      <h4 class="page__comments-title">Leave a Comment</h4>
      <section id="disqus_thread"></section>
    
</div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/2020/taylor-vick-M5tzZtFCOfs-unsplash-200.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/2020/popular-connectors-for-computers" rel="permalink">Popular connectors for computers
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="description">Pin out tables, some photos and parts of data come from Wikipedia.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/2019/jon-tyson-520955-unsplash-200.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/2019/a-decade-with-wikipedia" rel="permalink">A decade with Wikipedia
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="description">I’ve spent a decade with Wikipedia.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/2018/antenna-503044-unsplash-200.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/2018/how-to-have-a-data-science-lab" rel="permalink">How to have a data science lab in 13 steps?
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="description">TL;DR. You have to be passionate and insane. I would not recommend doing it.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/2018/gsoc-shirt-200.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/2018/how-you-start-gsoc-with-discourse" rel="permalink">3 steps to start Google Summer of Code with Discourse
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="description">Discourse is a wonderful free and open source software for a modern forum. Now it has already powered many projects’ forum. Take a look at where it’s built a...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 Erick Guan. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>







  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-36993160-8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-36993160-8', { 'anonymize_ip': false});
</script>






    
  <script>
    var disqus_config = function () {
      this.page.url = "https://erickguan.me/2019/pytorch-parallel-model";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/2019/pytorch-parallel-model"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://fftenacity.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  





  </body>
</html>
