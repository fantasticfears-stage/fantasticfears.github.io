<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.19.1 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>K-Nearest Neighbor Classification in Scikit Learn - Blog</title>
<meta name="description" content="K-Nearest Neighbor (k-NN) presents a a simple straightforward instance-based learning. Often, a simple strategy produces a good result as well as acting as baseline performance.  ">


  <meta name="author" content="Erick Guan">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Blog">
<meta property="og:title" content="K-Nearest Neighbor Classification in Scikit Learn">
<meta property="og:url" content="https://erickguan.me/2017/k-nearest-neighbour-in-scikit-learn">


  <meta property="og:description" content="K-Nearest Neighbor (k-NN) presents a a simple straightforward instance-based learning. Often, a simple strategy produces a good result as well as acting as baseline performance.  ">





  <meta name="twitter:site" content="@erickguan">
  <meta name="twitter:title" content="K-Nearest Neighbor Classification in Scikit Learn">
  <meta name="twitter:description" content="K-Nearest Neighbor (k-NN) presents a a simple straightforward instance-based learning. Often, a simple strategy produces a good result as well as acting as baseline performance.  ">
  <meta name="twitter:url" content="https://erickguan.me/2017/k-nearest-neighbour-in-scikit-learn">

  
    <meta name="twitter:card" content="summary">
    
  

  
    <meta name="twitter:creator" content="@erickguan">
  



  <meta property="article:published_time" content="2017-11-02T00:00:00+01:00">





  

  


<link rel="canonical" href="https://erickguan.me/2017/k-nearest-neighbour-in-scikit-learn">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Erick Guan",
      "url": "https://erickguan.me/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Blog Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#da532c">
<meta name="apple-mobile-web-app-title" content="Erick Guan">
<meta name="application-name" content="Erick Guan">
<meta name="theme-color" content="#ffffff">

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Blog
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about">About</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="K-Nearest Neighbor Classification in Scikit Learn">
    <meta itemprop="description" content="K-Nearest Neighbor (k-NN) presents a a simple straightforward instance-basedlearning. Often, a simple strategy produces a good result as well as acting asbaseline performance.">
    <meta itemprop="datePublished" content="2017-11-02T00:00:00+01:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">K-Nearest Neighbor Classification in Scikit Learn
</h1>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>K-Nearest Neighbor (k-NN) presents a a simple straightforward instance-based
learning. Often, a simple strategy produces a good result as well as acting as
baseline performance.</p>

<p>This article doesn’t deliver new knowledge but an interpretation and bridge to
others’ work. The reader need to understand the very basic of Machine Learning.
Especially, code is done with <code class="language-plaintext highlighter-rouge">scikit-learn</code>.</p>

<p>In particular, KNN can be used in classification. The training data is vector
in a multidimensional space with a class label. <code class="language-plaintext highlighter-rouge">k</code> is an user-defined constant.
A test sample is classified based on a distance metric with <code class="language-plaintext highlighter-rouge">k</code> nearest samples from
the training data. That distance metric can be Euclidean distance for continuous
variables. As of discrete data, Hamming distance is a good choice.</p>

<p>In Scholarpedia, it was visualized with Voronoi tessellation:</p>

<blockquote>
  <p>The k-nearest-neighbor classifier is commonly based on the Euclidean distance between a test sample and the specified training samples. Let <script type="math/tex">\mathbf{x}_i</script> be an input sample with <script type="math/tex">p</script> features <script type="math/tex">(\mathbf{x}_{i1},\mathbf{x}_{i2},\ldots,\mathbf{x}_{ip})</script>, <script type="math/tex">n</script> be the total number of input samples <script type="math/tex">(i=1,2,\ldots,n)</script> and <script type="math/tex">p</script> the total number of features <script type="math/tex">(j=1,2,\ldots,p)</script>. The Euclidean distance between sample <script type="math/tex">\mathbf{x}_i</script> and <script type="math/tex">\mathbf{x}_l (l=1,2,\ldots,n)</script> is defined as</p>

  <p><script type="math/tex">d(\mathbf{x}_i,\mathbf{x}_l)=\sqrt{(x_{i1}−x_{l1})^2+(x_{i2}−x_{l2})^2+\cdots+(x_{ip}−x_{lp})^2}</script>.</p>

  <p><img src="/assets/images/2017/knn-voronoi.png" alt="Voronoi tessellation showing Voronoi cells of 19 samples marked with a &quot;+&quot;. The Voronoi tessellation reflects two characteristics of the example 2-dimensional coordinate system: i) all possible points within a sample's Voronoi cell are the nearest neighboring points for that sample, and ii) for any sample, the nearest sample is determined by the closest Voronoi cell edge." /></p>

  <p>A graphic depiction of the nearest neighbor concept is illustrated in the Voronoi tessellation (Voronoi, 1907) shown in Figure. The tessellation shows 19 samples marked with a “+”, and the Voronoi cell, R , surrounding each sample. A Voronoi cell encapsulates all neighboring points that are nearest to each sample and is defined as</p>

  <p><script type="math/tex">R_i=\{\mathbf{x}\in\mathbb{R}^p:d(\mathbf{x},\mathbf{x}_i) \leq d(x,x_m),\forall i \neq m\}</script>,</p>

  <p>where <script type="math/tex">R_i</script> is the Voronoi cell for sample <script type="math/tex">\mathbf{x}_i</script>, and <script type="math/tex">x</script> represents all possible points within Voronoi cell <script type="math/tex">R_i</script>. Voronoi tessellations primarily reflect two characteristics of a coordinate system: i) all possible points within a sample’s Voronoi cell are the nearest neighboring points for that sample, and ii) for any sample, the nearest sample is determined by the closest Voronoi cell edge. Using the latter characteristic, the k-nearest-neighbor classification rule is to assign to a test sample the majority category label of its k nearest training samples. In practice, k is usually chosen to be odd, so as to avoid ties. The k = 1 rule is generally called the nearest-neighbor classification rule.</p>
</blockquote>

<p>This is a great description of understanding. However I found out it’s misleading
as visualization emphasizes too much on Voronoi instead of KNN itself.</p>

<h2 id="how-it-works">How it works</h2>

<p>It all comes out with the code. As in <code class="language-plaintext highlighter-rouge">scikit-learn</code>, the <code class="language-plaintext highlighter-rouge">neighbors.KNeighborsClassifier(n_neighbors, algorithm='brute')</code> implements the most simple way to use KNN. The class comprises of 4 mixins:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">SupervisedIntegerMixin</code>: a helper checks parameters and invoke real function.</li>
  <li><code class="language-plaintext highlighter-rouge">ClassifierMixin</code>: also a helper</li>
  <li><code class="language-plaintext highlighter-rouge">NeighborsBase</code>: this mixin choose the optimal algorithm for efficient computing</li>
  <li><code class="language-plaintext highlighter-rouge">KNeighborsMixin</code>: brute method is implemented here.</li>
</ul>

<p>We only have to see how the brute with euclidean distance works to understand it.</p>

<p><code class="language-plaintext highlighter-rouge">X</code> is the input here. <code class="language-plaintext highlighter-rouge">self._fit_X</code> is training data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_samples</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="n">sample_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span>
<span class="n">dist</span> <span class="o">=</span> <span class="n">pairwise_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_X</span><span class="p">,</span> <span class="s">'euclidean'</span><span class="p">)</span>
<span class="c1"># here X is compared with all training samples.
# dist is like a matrix of distance from X to training samples.
# Example: `array([[1, 3, 2, 4]])`, 1 sample with 4 training data.
</span>
<span class="n">neigh_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argpartition</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">n_neighbors</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># sort until `n_neighbors - 1`. When return, on the left side
# it should be smaller than this data point
</span><span class="n">neigh_ind</span> <span class="o">=</span> <span class="n">neigh_ind</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_neighbors</span><span class="p">]</span>
<span class="c1"># argpartition doesn't guarantee sorted order, so we sort again
</span><span class="n">neigh_ind</span> <span class="o">=</span> <span class="n">neigh_ind</span><span class="p">[</span>
    <span class="n">sample_range</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">dist</span><span class="p">[</span><span class="n">sample_range</span><span class="p">,</span> <span class="n">neigh_ind</span><span class="p">])]</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dist</span><span class="p">[</span><span class="n">sample_range</span><span class="p">,</span> <span class="n">neigh_ind</span><span class="p">]),</span> <span class="n">neigh_ind</span>
</code></pre></div></div>

<p>Staring from <code class="language-plaintext highlighter-rouge">neigh_ind</code> are some lines only used to sort result.
At last, a distance result and the order of distance to training
data is returned.</p>

<p>If you ever tried <a href="http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification">the sample code</a>, you would notice the
choice of <code class="language-plaintext highlighter-rouge">k</code> affects the result a lot upon iris dataset.</p>

<p>Plus, the weight metric and <code class="language-plaintext highlighter-rouge">RadiusNeighborsClassifier</code> is a
further enhancement.</p>

<h2 id="conclusion">Conclusion</h2>

<p>KNN is a really simple nearest neighbor classification or
regression tool. It’s nothing fancy but with weight and algorithm
you can tweak.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#artificial-intelligence" class="page__taxonomy-item" rel="tag">Artificial Intelligence</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#pattern-recognition" class="page__taxonomy-item" rel="tag">Pattern Recognition</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2017-11-02T00:00:00+01:00">November 2, 2017</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?via=erickguan&text=K-Nearest+Neighbor+Classification+in+Scikit+Learn%20https%3A%2F%2Ferickguan.me%2F2017%2Fk-nearest-neighbour-in-scikit-learn" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ferickguan.me%2F2017%2Fk-nearest-neighbour-in-scikit-learn" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ferickguan.me%2F2017%2Fk-nearest-neighbour-in-scikit-learn" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/2017/the-surprise-number-in-javascript" class="pagination--pager" title="The Surprising Number in JavaScript
">Previous</a>
    
    
      <a href="/2018/python-iterator" class="pagination--pager" title="Digging in Python iterator and enumerate
">Next</a>
    
  </nav>

    </div>

    
      <div class="page__comments">
  
  
      <h4 class="page__comments-title">Leave a Comment</h4>
      <section id="disqus_thread"></section>
    
</div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/2020/taylor-vick-M5tzZtFCOfs-unsplash-200.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/2020/popular-connectors-for-computers" rel="permalink">Popular connectors for computers
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="description">Pin out tables, some photos and parts of data come from Wikipedia.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/2019/jon-tyson-520955-unsplash-200.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/2019/a-decade-with-wikipedia" rel="permalink">A decade with Wikipedia
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="description">I’ve spent a decade with Wikipedia.

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/2019/Pytorch_logo-200.png" alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/2019/pytorch-parallel-model" rel="permalink">How PyTorch implements DataParallel?
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="description">PyTorch can send batches and models to different GPUs automatically with DataParallel(model). How is it possible? I assume you know PyTorch uses dynamic comp...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/2018/antenna-503044-unsplash-200.jpg" alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/2018/how-to-have-a-data-science-lab" rel="permalink">How to have a data science lab in 13 steps?
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="description">TL;DR. You have to be passionate and insane. I would not recommend doing it.

</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 Erick Guan. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>







  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-36993160-8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-36993160-8', { 'anonymize_ip': false});
</script>






    
  <script>
    var disqus_config = function () {
      this.page.url = "https://erickguan.me/2017/k-nearest-neighbour-in-scikit-learn";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/2017/k-nearest-neighbour-in-scikit-learn"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://fftenacity.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  





  </body>
</html>
